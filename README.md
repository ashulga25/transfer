Repo Description:

Sample python hello-world web-app, which has 3 endpoints

- /             # Prints "Hello, World!"
- healthz       # healthcheck
- /primes/<n>   # prints specified number of prime numbers. Purpose - simulate CPU load and cause auto-scaling.
                # A proper number to check auto-scaling is working properly would be about 125000 or more.
                # If the very large number calculation is initiated - it would generate a 504 Gateway timeout, 
                but the app would continue calculations.


Pipeline is build in a way to use Azure Federated OIDC. 
So upon deployment just update following from TF output
Secrets:
- ACR_NAME                      # Azure ACR NAME 
- AZURE_CLIENT_ID               # Azure GitHubActionsOIDC AppID generated via command
                                # az ad app create --display-name GitHubActionsOIDC
- AZURE_SUBSCRIPTION_ID         # The subscription where app is deployed
- AZURE_TENANT_ID               # Azure Tenant_ID required for authentication
- RESOURCE_GROUP                # Resource group where acr and webapp are located 
- WEBAPP_NAME                   # Name of Azure web application to be used when deploying into cloud

Variables:
- APP_NAME                  # Appllication name, required for building container app and tagging. Default name "python-app"
- ACR_REPO_NAME             # Azure ACR Repo name, default `transfer-app`
- QUALITY_GATE              # Number of acceptable trivy critical findings. If exceeded - pipeline would break. Default `5`
- SITE_TESL_URL             # URL name where the app should be reachable, used for test upon deployment. 
                            # Set to `transfer-ohkpin-webapp.azurewebsites.net`


Pipeline builds the docker image, performs tests of the built application with basic tests.
Also pipeline performs vulnerability scan with trivy and prints the table of HIGH and CRITICAL findings.
Also it has some predefined threshold, if number of CRITICAL findings is too high - the pipeline would break, forcing developer to fix vulnerabilities in the app.
Scan results are stored as artifact and can be downloaded by users for offline analysis.


Push to Azure ACR and deployment of the application into Azure WebApp are performed using Azure Federated OIDC approach, so no credentials are stored in the repo, only the client_id, tenant_id, subscription_id. All of them are stored as secrets. This is the most secure approach, as tokens used are short-lived and Azure IAM permissions are very granular and allow access only from this github_organization and repo. 
Pipeline authenticates via OIDC in Azure IAM, so we have to create respective App account and federated setting in Azure Organization.

A new image tag is used for every deployment, based on built. Only app repository remains static with this approach.

Pipeline utilizes a static app name generated by Azure for tests to check that application is reachable and the `/healthz` endpoint responds the status.
There is some delay until the app would restart after deploy, so test is utilizing a script with 10 attempts to check endpoint availability, sleeping 60s between attempts.

I have not registered any custom domain name, as it would be extra paid service, so no custom TLS certificate 
associated/deployed into KeyVault as well. This can be a point for improvement.

Also, we have a simple app, which does not transfer any sensitive data.

.dockerfile ignore is used for the Docker to add only application relevant files to the image, and do not add the kubernetes deployment of github pipeline files.


Architecture is quite simple:
- WebApp service runs inside VNET with NSG, which filters out connections other than on port 443. 
- Azure ACR is used to store the images built via GitHubActions pipeline.
- WebApp and ACR are SUPPOSED to communicate via vnet, ACR is connected via private endpoint.
    Yet it is not mandatory, as we push into ACR via internet from GitHub. So we could pull the same way.
    In order to authenticate - the user-managed identity is used with `ACRPull` role.
    Also we can switch to system-assigned identity.
    Upon initial deploy the WebApp used nginx-alpine image and then upon run of GitHub pipeline it would be
    replaced with the application image.
- Log Workspace is configured, so WebApp reports the logs into it.
- Fot the code to re-deployable and have unique name with less possiblity of errors during re-deploy,
  I used the random provider to generate some random suffix.


Issues:
WebApp fails to properly pull image via private endpoint, and I could not troubleshooot it fully. Wasted several
hours on it. Looks like using managed-identity over the private endpoint does not work properly.
There is a proper private DNS record with proper IP and service endpoint enabled, still webapp fails
to pull image over the private endpoint.


Improvement areas:
1) Add FrontDoor with Azure WAF with some firewall rules
2) Add some extra metrics for the Log Workspace
3) Add autoscaling for the web-app based on CPU/RAM/Number of requests
4) Automate infrastructure deployment, so the AD application for GitHub and extra permissions would be created as part of infrastructure deployment.
5) Improve Terraform output